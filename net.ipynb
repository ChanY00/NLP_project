{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b0cd386",
   "metadata": {},
   "source": [
    "## 신경망 모델(HGNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bad57c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dnjsr\\AppData\\Local\\Temp\\ipykernel_95564\\2789984928.py:13: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  train_data = pd.read_sql(query, db_connection)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         patent_name   \n",
      "0                                    파장변환 및 클락 추출 방법  \\\n",
      "1                    사용자 및 디바이스 기반의 도메인 관리 방법 및 그 장치   \n",
      "2  대기상태 전원 및 구동전압 제어기 및 그를 이용한 고주파 이동통신 송수신 회로 및 시스템   \n",
      "3                           정보보호를 위한 알에프아이디 시스템 및 방법   \n",
      "4                       무선 휴대 인터넷 시스템에서 채널 모드간 전환 방법   \n",
      "\n",
      "                      processed_patent_name country  application_year  \n",
      "0                                파장변환 추출 방법      미국              2006  \n",
      "1                  사용자 디바이스 기반 도메인 관리 방법 장치      미국              2007  \n",
      "2  대기상태 전원 구동전압 제어기 그 이용 고주파 이동통신 송수신 회 시스템      미국              2006  \n",
      "3                        정보보호 알에프아이디 시스템 방법      미국              2006  \n",
      "4                무선 휴대 인터넷 시스템 채널 모드간 전환 방법      미국              2006  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import mysql.connector\n",
    "\n",
    "db_connection = mysql.connector.connect(\n",
    "    host='127.0.0.1',\n",
    "    user='root',\n",
    "    password='',\n",
    "    db='hyperanalystic',\n",
    ")\n",
    "\n",
    "# 데이터 가져오기\n",
    "query = \"SELECT patent_name, processed_patent_name, country, application_year FROM train_predict_data;\"\n",
    "train_data = pd.read_sql(query, db_connection)\n",
    "\n",
    "# 연결 종료\n",
    "db_connection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bb633f",
   "metadata": {},
   "source": [
    "#### 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7251ff71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "필터링된 단어: ['방', '법', '이', '장', '치', '스', '시', '신', '기', '용', '호', '템', '화', '전', '터', '송', '상', '제', '리', '수', '트', '통', '선', '정', '부', '성', '자', '동', '지', '반', '무', '조', '다', '디', '영', '인', '적', '레', '보', '비', '크', '복', '오', '중', '서', '드', '파', '프', '구', '광', '체', '데', '그', '원', '대', '모', '사', '형', '공', '링', '소', '널', '차', '단', '로', '향', '가', '생', '간', '측', '채', '식', '계', '변', '워', '력', '분', '어', '속', '네', '고', '코', '출', '라', '관', '역', '버', '위', '주', '세', '안', '포', '경', '율', '유', '도', '환', '패', '테', '예', '저', '행', '랜', '처', '합', '티', '응', '작', '딩', '메', '미', '능', '플', '접', '나', '연', '검', '하', '록', '음', '블', '추', '센', '산', '한', '할', '재', '효', '물', '실', '색', '매', '발', '캐', '증', '함', '멀', '감', '탐', '에', '러', '킷', '층', '우', '입', '망', '면', '결', '당', '브', '운', '셀', '량', '바', '내', '회', '마', '츠', '션', '교', '일', '노', '길', '절', '설', '질', '직', '케', '객', '국', '벡', '텐', '양', '콘', '태', '타', '털', '개', '집', '폭', '초', '래', '액', '거', '더', '피', '류', '말', '압', '듈', '임', '점', '픽', '현', '석', '토', '배', '클', '룰', '넷', '컴', '것', '학', '심', '술', '확', '진', '밍', '표', '필', '판', '렬', '명', '근', '종', '퓨', '섭', '의', '택', '아', '득', '열', '컨', '핑', '과', '징', '커', '격', '핸', '별', '순', '특', '각', '팅', '게', '페', '박', '축', '막', '휴', '웨', '셋', '협', '해', '볼', '니', '릴', '카', '너', '델', '칭', '빙', '업', '강', '닝', '평', '퍼', '습', '활', '최', '줄', '문', '키', '루', '림', '품', '극', '홀', '들', '날', '베', '요', '온', '달', '투', '편', '싱', '램', '슬', '얼', '엑', '즈', '등', '백', '획', '맵', '약', '턴', '병', '청', '빔', '린', '밀', '참', '덤', '료', '금', '항', '애', '암', '존', '충', '헤', '롯', '립', '굴', '렌', '규', '허', '콜', '황', '외', '잡', '봇', '칩', '악', '튜', '불', '렉', '촉', '앰', '혼', '쉬', '알', '폴', '뢰', '균', '후', '덱', '준', '련', '컬', '번', '쿼', '룹', '궤', '험', '뮬', '급', '펄', '킹', '런', '곡', '탑', '럴', '손', '답', '여', '건', '르', '믹', '촬', '웹', '랫', '머', '폼', '홈', '먼', '독', '럿', '엔', '융', '착', '랙', '벤', '름', '듀', '언', '잔', '섬', '맷', '목', '률', '닛', '즘', '릭', '펙', '닉', '훈', '천', '틱', '텍', '및', '돌', '범', '맞', '춤', 'i', '핀', '값', '넌', '난', 'd', '깊', '앙', 'o', '럭', '히', '벨', '큐', '를', '롤', '왜', '펨', '캡', '쥴', '퀀', '억', '럼', '슐', '흡', '휘', 'm', '튬', '엠', '락', '폐', '본', 'a', '폰', '윈', '뉴', '론', '완', '덕', '첩', '옵', '군', '탄', 'l', '젠', '샘', 'n', '두', '홉', '침', '삼', '귀', '킵', '푸', '웃', '캔', 't', '뎀', '책', '셜', 'e', 's', '던', '쇄', '야', 'c', '령', 'r', '솔', '캘', '텔', 'p', '갱', '삽', '젝', '권', '밴', '첨', '취', '을', '와', '북', '탈', '엘', '창', '익', '좌', '펌', '몰', '란', '릿', '씨', '만', '칙', 'w', '냉', '맥', '셈', '혹', '쟁', '넥', 'f', '람', '쇼', '칼', '섹', '육', '누', '펑', '승', '쳐', '딥', '된', '빅', '울', '긴', '륨', '벌', '견', '룩', '혈', '논', '톱', '슈', '깅', '쓰', '녹', '늄', '글', '깃', '늘', '염', '빌', '뷰', '철', '맨', '냅', '골', '친', '겟', '듐', '낭', 'u', '낮', '은', '즌', '뇨', '샷', '홍', '랩', '벽', '봉', '농', '쇠', 'h', '몬', '픈', '멤', '틸', '셔', '잉', '낙', '톨', '쌍', '눅', 'v', '햅', '콤', '덴', '으', '멘', 'g', '족', '곱', 'b', '엣', '팩', '뇌', '톤', '겸', '례', '둔', '쉐', '갭', '는', '츄', '귤', '헬', '곽', '횡', '또', '텀', '큰', '넬', '써', '희', '틴', '든', '밸', '찰', '팜', '댑', '걸', '삭', '괄', '챗', '옥', '셉', '민', '멸', '핫', '져', '냄', '새', '눈', '룬', '닐', '핵', '튼', '겐', '갈', '핏', '닥', '윙', '탬', '랭', '멍', '랑', '총']\n",
      "필터링된 단어 수: 615\n",
      "빈 리스트: 0\n",
      "Encoded shape: (13851, 616)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# 단어 빈도 계산\n",
    "all_words = [word for words in train_data['processed_patent_name'] for word in words]\n",
    "word_freq = pd.Series(all_words).value_counts()\n",
    "\n",
    "# 빈도 조건 설정\n",
    "min_freq = 3  # 최소 빈도 조건\n",
    "max_freq = 0.95 * len(train_data)  # 최대 빈도 조건\n",
    "\n",
    "# 조건에 맞는 단어 필터링\n",
    "filtered_words = word_freq[(word_freq >= min_freq) & (word_freq <= max_freq)].index.tolist()\n",
    "\n",
    "# 필터링 결과 확인\n",
    "print(f\"필터링된 단어: {filtered_words}\")\n",
    "print(f\"필터링된 단어 수: {len(filtered_words)}\")\n",
    "\n",
    "# 'processed_patent_name' 컬럼 업데이트\n",
    "train_data['processed_patent_name'] = train_data['processed_patent_name'].apply(\n",
    "    lambda words: [word for word in words if word in filtered_words]\n",
    ")\n",
    "\n",
    "# 필터링 후 빈 리스트를 대체\n",
    "train_data['processed_patent_name'] = train_data['processed_patent_name'].apply(\n",
    "    lambda words: words if len(words) > 0 else ['unknown']\n",
    ")\n",
    "\n",
    "# 결과 확인\n",
    "empty_rows = train_data[train_data['processed_patent_name'].apply(len) == 0]\n",
    "print(f\"빈 리스트: {len(empty_rows)}\")\n",
    "\n",
    "# 인코딩\n",
    "mlb = MultiLabelBinarizer()\n",
    "encoded_data = mlb.fit_transform(train_data['processed_patent_name'])\n",
    "print(f\"Encoded shape: {encoded_data.shape}\")\n",
    "\n",
    "# 데이터 변환 결과 저장\n",
    "train_data['encoded_patent_name'] = list(encoded_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5711af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Train Loss: 0.5147480368614197, Validation Loss: 0.2694413959980011\n",
      "Epoch 10, Train Loss: 0.25180816650390625, Validation Loss: 0.267046183347702\n",
      "Epoch 20, Train Loss: 0.260055810213089, Validation Loss: 0.26516908407211304\n",
      "Epoch 30, Train Loss: 0.2495841532945633, Validation Loss: 0.25743842124938965\n",
      "Epoch 40, Train Loss: 0.24594317376613617, Validation Loss: 0.2541368305683136\n",
      "Epoch 50, Train Loss: 0.24522702395915985, Validation Loss: 0.25378715991973877\n",
      "Epoch 60, Train Loss: 0.24496588110923767, Validation Loss: 0.25548964738845825\n",
      "Epoch 70, Train Loss: 0.24202609062194824, Validation Loss: 0.2543905973434448\n",
      "Epoch 80, Train Loss: 0.2395016998052597, Validation Loss: 0.25426480174064636\n",
      "Epoch 90, Train Loss: 0.23490482568740845, Validation Loss: 0.2551967203617096\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import HypergraphConv, global_mean_pool\n",
    "from torch_geometric.data import Data\n",
    "from torch.optim import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "\n",
    "# 하이퍼그래프 모델 정의\n",
    "class HGNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(HGNN, self).__init__()\n",
    "        self.layer1 = HypergraphConv(input_dim, hidden_dim)\n",
    "        self.layer2 = HypergraphConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, edge_index, batch=None):\n",
    "        if edge_index.size(1) == 0:\n",
    "            print(\"빈 엣지 인덱스\")\n",
    "            return x\n",
    "        x = F.relu(self.layer1(x, edge_index))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.layer2(x, edge_index)\n",
    "        if batch is not None:\n",
    "            x = global_mean_pool(x, batch)\n",
    "        return x\n",
    "\n",
    "# 데이터셋 분할\n",
    "train_ratio = 0.6\n",
    "val_ratio = 0.2\n",
    "test_ratio = 0.2\n",
    "\n",
    "train_indices, test_indices = train_test_split(range(len(train_data)), test_size=test_ratio, random_state=42)\n",
    "train_indices, val_indices = train_test_split(train_indices, test_size=val_ratio / (train_ratio + val_ratio), random_state=42)\n",
    "\n",
    "train_data_split = train_data.iloc[train_indices]\n",
    "val_data_split = train_data.iloc[val_indices]\n",
    "test_data_split = train_data.iloc[test_indices]\n",
    "\n",
    "# 하이퍼그래프 데이터 생성 함수\n",
    "def create_hypergraph_data(data_split, mlb, num_nodes):\n",
    "    edges = []\n",
    "    for idx, row in data_split.iterrows():\n",
    "        patent_name = row['processed_patent_name']\n",
    "        for word in patent_name:\n",
    "            try:\n",
    "                word_index = mlb.classes_.tolist().index(word)\n",
    "                edges.append((word_index, idx))\n",
    "            except ValueError as e:\n",
    "                print(f\"Warning: Word '{word}' not in MultiLabelBinarizer classes.\")\n",
    "    if edges:\n",
    "        edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "    else:\n",
    "        edge_index = torch.empty((2, 0), dtype=torch.long)\n",
    "    x = torch.eye(num_nodes, dtype=torch.float)\n",
    "    return Data(x=x, edge_index=edge_index)\n",
    "\n",
    "num_nodes = len(mlb.classes_)\n",
    "train_data_obj = create_hypergraph_data(train_data_split, mlb, num_nodes)\n",
    "val_data_obj = create_hypergraph_data(val_data_split, mlb, num_nodes)\n",
    "test_data_obj = create_hypergraph_data(test_data_split, mlb, num_nodes)\n",
    "\n",
    "# 모델 초기화\n",
    "model = HGNN(input_dim=num_nodes, hidden_dim=64, output_dim=2)\n",
    "optimizer = Adam(model.parameters(), lr=0.04)\n",
    "\n",
    "# 타겟 생성\n",
    "train_target = torch.randint(0, 2, (train_data_obj.num_nodes, 2)).float()\n",
    "val_target = torch.randint(0, 2, (val_data_obj.num_nodes, 2)).float()\n",
    "test_target = torch.randint(0, 2, (test_data_obj.num_nodes, 2)).float()\n",
    "\n",
    "# 학습 및 검증\n",
    "def train_with_validation(train_data, val_data, model, optimizer, train_target, val_target, epochs=100):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(train_data.x, train_data.edge_index)\n",
    "        train_loss = F.mse_loss(out, train_target)\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_out = model(val_data.x, val_data.edge_index)\n",
    "            val_loss = F.mse_loss(val_out, val_target)\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}, Train Loss: {train_loss.item()}, Validation Loss: {val_loss.item()}\")\n",
    "\n",
    "train_with_validation(train_data_obj, val_data_obj, model, optimizer, train_target, val_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e308b5a1",
   "metadata": {},
   "source": [
    "#### 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e18ff17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 48: Patent Name: 다중 사용자 무선 통신 시스템에서 제어 및 훈련 심볼 전송 방법, Year: 2018, Country: 대한민국, Potential Score: 0.6128\n",
      "Rank 113: Patent Name: 영상 부호화 및 복호화를 위한 장치 및 방법, Year: 2017, Country: 대한민국, Potential Score: 0.6011\n",
      "Rank 153: Patent Name: 중앙 집중식 매체 접속 제어 프로토콜을 사용하는 광대역 고주파수 무선 시스템에서 우회 경로 설정 방법 및 그 장치, Year: 2019, Country: 미국, Potential Score: 0.5700\n",
      "Rank 167: Patent Name: 통합 음성/음악 신호 부/복호화 방법 및 장치, Year: 2018, Country: 미국, Potential Score: 0.5565\n",
      "Rank 163: Patent Name: 초고주파 송수신 장치, Year: 2019, Country: 미국, Potential Score: 0.5513\n",
      "Rank 142: Patent Name: 적응적 스캐닝을 이용한 동영상 부호화/복호화 장치 및 그 방법, Year: 2018, Country: 미국, Potential Score: 0.5484\n",
      "Rank 5: Patent Name: 60GHz 대역을 사용하는 LOS 통신시스템에서 중계 장치를 경유하여로 우회 경로를 통한 데이터 송수신 방법 및 그 장치, Year: 2018, Country: 미국, Potential Score: 0.5471\n",
      "Rank 11: Patent Name: MBMS 서비스의 동적 스케줄링 지원 방법 및 절차, Year: 2018, Country: 미국, Potential Score: 0.5356\n"
     ]
    }
   ],
   "source": [
    "# 상위 특허 추출\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out = model(train_data_obj.x, train_data_obj.edge_index)\n",
    "\n",
    "predicted_scores = out[:, 1]\n",
    "top_10_indices = predicted_scores.argsort(descending=True)[:200]\n",
    "top_10_patents = train_data.iloc[top_10_indices].copy()\n",
    "top_10_scores = predicted_scores[top_10_indices]\n",
    "top_10_patents['score'] = top_10_scores.numpy()\n",
    "\n",
    "top_10_patents_unique = top_10_patents.groupby('patent_name').agg({\n",
    "    'application_year': 'first',\n",
    "    'country': 'first',\n",
    "    'score': 'max',\n",
    "}).reset_index()\n",
    "\n",
    "top_10_patents_unique_filtered = top_10_patents_unique[top_10_patents_unique['application_year'] > 2016]\n",
    "top_10_patents_unique_filtered = top_10_patents_unique_filtered.sort_values(by='score', ascending=False)\n",
    "\n",
    "if top_10_patents_unique_filtered.empty:\n",
    "    print(\"특허가 존재하지 않음\")\n",
    "else:\n",
    "    for idx, row in top_10_patents_unique_filtered.iterrows():\n",
    "        print(f\"Rank {idx + 1}: Patent Name: {row['patent_name']}, Year: {row['application_year']}, \"\n",
    "              f\"Country: {row['country']}, Potential Score: {row['score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a008b1",
   "metadata": {},
   "source": [
    "## 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48db821b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.2583553194999695\n",
      "Mean Absolute Error (MAE): 0.5005\n",
      "Accuracy: 0.4951\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, r2_score, accuracy_score, precision_score, recall_score, f1_score\n",
    "import torch\n",
    "\n",
    "# 테스트 데이터 평가 함수 (MAE, Accuracy)\n",
    "def test(test_data, model, test_target):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(test_data.x, test_data.edge_index) \n",
    "        \n",
    "        test_loss = F.mse_loss(out, test_target)\n",
    "        print(f\"Test Loss: {test_loss.item()}\")\n",
    "\n",
    "        # 예측 값과 실제 값 변환\n",
    "        y_true = test_target.numpy()  # 실제 값\n",
    "        y_pred = out.numpy()         # 예측 값\n",
    "\n",
    "        # MAE 계산\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "\n",
    "        # Accuracy\n",
    "        # 예측 클래스와 실제 클래스 비교\n",
    "        predicted_class = torch.argmax(out, dim=1)  # 가장 큰 값이 예측 클래스\n",
    "        y_true_classes = y_true.argmax(axis=1)      # 실제 클래스\n",
    "\n",
    "        accuracy = accuracy_score(y_true_classes, predicted_class.numpy())\n",
    "        # 평가 결과 출력\n",
    "        print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# 테스트 평가 실행\n",
    "test(test_data_obj, model, test_target)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
